namespaceOverride: monitoring
nameOverride: kube-prometheus-stack
fullnameOverride: kube-prometheus-stack

global:
  rbac:
    create: true

crds:
  enabled: true
  upgradeJob:
    enabled: false

# ================================
# ETCD Scraping for Talos
# ================================
kubeEtcd:
  endpoints:
    - 192.168.101.120
    - 192.168.101.121
    - 192.168.101.122
  service:
    selector:
      component: etcd
  serviceMonitor:
    scheme: https
    insecureSkipVerify: true
    serverName: localhost
    caFile: /etc/prometheus/secrets/etcd-certs/etcd-ca.crt
    certFile: /etc/prometheus/secrets/etcd-certs/etcd-client.crt
    keyFile: /etc/prometheus/secrets/etcd-certs/etcd-client-key.key

# ================================
# Control Plane Components
# ================================
kubeControllerManager:
  endpoints:
    - 192.168.101.120
    - 192.168.101.121
    - 192.168.101.122

kubeScheduler:
  endpoints:
    - 192.168.101.120
    - 192.168.101.121
    - 192.168.101.122

# ================================
# Prometheus Configuration
# ================================
prometheus:
  prometheusSpec:
    remoteWrite:
      - url: http://mimir-nginx.monitoring.svc.cluster.local:80/api/v1/push
        headers:
          X-Scope-OrgID: "anonymous"
        # Add write relabel configs to limit metrics
        writeRelabelConfigs:
          # Only send important metrics, filter out high-cardinality ones
          - sourceLabels: [__name__]
            action: keep
            regex: (node_.*|kube_.*|container_.*|apiserver_.*|etcd_.*|kubelet_.*|up|rate|latency)
          # Exclude some high-cardinality metrics
          - sourceLabels: [__name__]
            action: drop
            regex: (container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|.+_bucket)
        # Add headers for Mimir authentication
        headers:
          X-Scope-OrgID: "1" # Use the default anonymous org ID
        writeRelabelConfigs:
          # Initially include everything - you can limit later if needed
          - sourceLabels: [ __name__ ]
            action: keep
            regex: (.+)
    secrets:
      - etcd-client-certs
    extraVolumes:
      - name: etcd-certs
        secret:
          secretName: etcd-client-certs
    extraVolumeMounts:
      - name: etcd-certs
        mountPath: /etc/prometheus/secrets/etcd-certs
        readOnly: true
    podDisruptionBudget:
      enabled: true
      minAvailable: 1
    scrapeInterval: 15s
    evaluationInterval: 30s
    replicas: 2
    resources:
      requests:
        memory: 2Gi
        cpu: 1000m
      limits:
        memory: 4Gi
        cpu: 2000m
    retention: 15d
    retentionSize: 90GB
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector:
      matchLabels:
        release: kube-prometheus-stack
    serviceMonitorNamespaceSelector: {}
    securityContext:
      fsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    extraVolumes:
      - name: etcd-certs
        secret:
          secretName: etcd-client-certs
    extraVolumeMounts:
      - name: etcd-certs
        mountPath: /etc/prometheus/secrets/etcd-certs
        readOnly: true
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-block
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 30Gi
  ingress:
    enabled: true
    ingressClassName: internal
    annotations:
      kubernetes.io/ingress.class: internal
      cert-manager.io/cluster-issuer: "letsencrypt-production"
    hosts:
      - prometheus.wakkalabs.com
    path: /
    pathType: Prefix
    tls:
      - secretName: prometheus-wakkalabs-tls
        hosts:
          - prometheus.wakkalabs.com

# ================================
# Alertmanager Configuration
# ================================
alertmanager:
  enabled: true # This is default, but good to be explicit
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  replicas: 1
  service:
    ipFamilyPolicy: SingleStack
    ipFamilies:
      - IPv4
  ingress:
    enabled: true
    ingressClassName: internal
    annotations:
      kubernetes.io/ingress.class: internal
      cert-manager.io/cluster-issuer: "letsencrypt-production"
    hosts:
      - alertmanager.wakkalabs.com
    path: /
    pathType: Prefix
    tls:
      - secretName: alertmanager-wakkalabs-tls
        hosts:
          - alertmanager.wakkalabs.com

  # Optional but recommended: Add persistent storage
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-block
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 10Gi

# ================================
# Default Rules Configuration
# ================================
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    kubeApiserver: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubernetesSystem: true
    kubernetesResources: true
    kubeControllerManager: true
    kubeScheduler: true
    kubernetesStorage: true
    prometheusOperator: true
  disabled:
    KubeControllerManagerDown: false
    KubeSchedulerDown: false
    KubeletTooManyPods: true

# ================================
# Grafana Configuration
# ================================
grafana:
  admin:
    existingSecret: grafana-admin-credentials
    userKey: admin-user
    passwordKey: admin-password
  enabled: true
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: "America/Chicago"
  defaultDashboardsEditable: true
  serviceMonitor:
    enabled: true
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      folder: /tmp/dashboards
      searchNamespace: ALL
      provider:
        foldersFromFilesStructure: true
        allowUiUpdates: true  # This is critical for fixing permissions
    datasources:
      enabled: true
      label: grafana_datasource
      searchNamespace: ALL

  persistence:
    enabled: true
    type: pvc
    accessModes:
      - ReadWriteOnce
    size: 30Gi
    storageClassName: ceph-block

  # Fix the security context to ensure proper permissions
  securityContext:
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472

  # Fix ingress configuration
  ingress:
    enabled: true
    ingressClassName: internal
    annotations:
      kubernetes.io/ingress.class: internal
      cert-manager.io/cluster-issuer: "letsencrypt-production"
    hosts:
      - grafana.wakkalabs.com
    path: /
    pathType: Prefix
    tls:
      - secretName: grafana-wakkalabs-tls
        hosts:
          - grafana.wakkalabs.com
  extraConfigmapMounts:
    - name: grafana-home-dashboard
      mountPath: /var/lib/grafana/dashboards/default/home-dashboard.json
      subPath: home-dashboard.json
      configMap: grafana-home-dashboard
      readOnly: true

  # Add datasources with explicit UIDs to prevent conflicts
  additionalDataSources:
    - name: Loki
      type: loki
      uid: loki_uid  # Use explicit UIDs to avoid conflicts
      access: proxy
      url: http://loki-gateway.monitoring.svc.cluster.local:80
      jsonData:
        maxLines: 1000
        timeout: 60
    - name: Tempo
      type: tempo
      uid: tempo_uid  # Use explicit UIDs to avoid conflicts
      access: proxy
      url: http://tempo.monitoring.svc.cluster.local:3100
      jsonData:
        httpMethod: GET
        lokiSearch:
          datasourceUid: loki_uid
        tracesToLogs:
          datasourceUid: loki_uid
          tags: [ 'instance', 'pod', 'namespace', 'service.name' ]
          mappedTags: [ { key: 'service.name', value: 'service' } ]
          mapTagNamesEnabled: true
          spanStartTimeShift: '-1h'
          spanEndTimeShift: '1h'
          filterByTraceID: true
          filterBySpanID: true
        serviceMap:
          datasourceUid: prometheus_uid
        nodeGraph:
          enabled: true
        search:
          hide: false
    # Additional Prometheus datasource with explicit UID
    - name: Prometheus
      type: prometheus
      uid: prometheus_uid  # Use explicit UIDs to avoid conflicts
      access: proxy
      url: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
      isDefault: true

  # Refined dashboard providers with correct permissions
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          updateIntervalSeconds: 30
          allowUiUpdates: true  # Critical for fixing permissions
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
        - name: 'kubernetes'
          orgId: 1
          folder: 'Kubernetes'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 30
          allowUiUpdates: true  # Critical for fixing permissions
          editable: true
          options:
            path: /var/lib/grafana/dashboards/kubernetes
        - name: 'network'
          orgId: 1
          folder: 'Network'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 30
          allowUiUpdates: true  # Critical for fixing permissions
          editable: true
          options:
            path: /var/lib/grafana/dashboards/network
        - name: 'storage'
          orgId: 1
          folder: 'Storage'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 30
          allowUiUpdates: true  # Critical for fixing permissions
          editable: true
          options:
            path: /var/lib/grafana/dashboards/storage
        - name: 'logging'
          orgId: 1
          folder: 'Logging'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 30
          allowUiUpdates: true  # Critical for fixing permissions
          editable: true
          options:
            path: /var/lib/grafana/dashboards/logging

    # Pyroscope datasource commented out until Pyroscope is deployed
    # - name: Pyroscope
    #   type: grafana-pyroscope-datasource
    #   uid: pyroscope
    #   access: proxy
    #   url: http://pyroscope.monitoring.svc.cluster.local:4040
    #   jsonData:
    #     minStep: 10s
    # Dashboard Providers

  # Dashboards
  dashboards:
    default:
      argocd-operational-overview:
        gnetId: 19993
        revision: 3
        datasource: Prometheus
      argocd-application-overview:
        gnetId: 19974
        revision: 3
        datasource: Prometheus
      kubernetes-cluster-monitoring:
        gnetId: 13770
        revision: 1
        datasource: Prometheus
      proxmox-overview:
        gnetId: 10048
        revision: 4
        datasource: Prometheus
      kubernetes-dashboard:
        gnetId: 15757
        revision: 1
        datasource: Prometheus

    kubernetes:
      k8s-node-rsrc-use:
        gnetId: 8171
        revision: 1
        datasource: Prometheus
      kubernetes-apiserver:
        gnetId: 12006
        revision: 1
        datasource: Prometheus
      kubelet-monitoring:
        gnetId: 14282
        revision: 1
        datasource: Prometheus
      node-exporter-full:
        gnetId: 1860
        revision: 31
        datasource: Prometheus
      kubernetes-cluster-monitoring-detailed:
        gnetId: 8588
        revision: 1
        datasource: Prometheus
      kubernetes-pod-monitoring:
        gnetId: 6417
        revision: 1
        datasource: Prometheus
      kubernetes-workload:
        gnetId: 15758
        revision: 1
        datasource: Prometheus
      kubernetes-controller-manager:
        gnetId: 12110
        revision: 1
        datasource: Prometheus
      kubernetes-scheduler:
        gnetId: 12130
        revision: 1
        datasource: Prometheus

    network:
      kubernetes-network-monitoring:
        gnetId: 12125
        revision: 1
        datasource: Prometheus
      nginx-ingress-controller:
        gnetId: 9614
        revision: 1
        datasource: Prometheus
      cilium-overview:
        gnetId: 6658
        revision: 1
        datasource: Prometheus

    storage:
      ceph-cluster:
        gnetId: 2842
        revision: 17
        datasource: Prometheus
      ceph-pools:
        gnetId: 5336
        revision: 9
        datasource: Prometheus
      kubernetes-persistent-volumes:
        gnetId: 13646
        revision: 2
        datasource: Prometheus

    # Add dedicated logging dashboards folder
    logging:
      # Keep Loki dashboards since Loki is deployed
      loki-logs:
        gnetId: 15141
        revision: 1
        datasource: Loki
      kubernetes-logs:
        gnetId: 15365
        revision: 1
        datasource: Loki
      loki-dashboard:
        gnetId: 12019
        revision: 2
        datasource:
          - name: DS_LOKI
            value: Loki
          - name: DS_PROMETHEUS
            value: Prometheus

kubeStateMetrics:
  enabled: true
  selfMonitor:
    enabled: true

# Keep Loki disabled since you're deploying it separately
loki:
  enabled: false
