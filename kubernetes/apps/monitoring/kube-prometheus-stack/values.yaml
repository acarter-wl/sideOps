namespaceOverride: monitoring
nameOverride: kube-prometheus-stack
fullnameOverride: kube-prometheus-stack

global:
  rbac:
    create: true
    pspEnabled: false # PSPs are deprecated in Kubernetes 1.25+

crds:
  enabled: true
  # Enable upgrade job for better CRD management during upgrades
  upgradeJob:
    enabled: true
    annotations:
      helm.sh/hook: pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded

# ================================
# ETCD Scraping for Talos
# ======================
kubeEtcd:
  enabled: true
  endpoints:
  - 192.168.101.120
  - 192.168.101.121
  - 192.168.101.122
  serviceMonitor:
    scheme: https
    insecureSkipVerify: false
    serverName: localhost
    caFile: /etc/prometheus/secrets/etcd-certs/etcd-ca.crt
    certFile: /etc/prometheus/secrets/etcd-certs/etcd-client.crt
    keyFile: /etc/prometheus/secrets/etcd-certs/etcd-client-key.key
    # Add RBAC to ensure etcd scraping works properly
    relabelings:
    - sourceLabels: [ __meta_kubernetes_pod_node_name ]
      targetLabel: node
      action: replace

prometheus:
  enabled: true
  scrapeInterval: 30s
  evaluationInterval: 30s
  # Properly scale with pod resources and add PodDisruptionBudget
  replicas: 2 # For high availability
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1
  resources:
    requests:
      memory: 2Gi
      cpu: 1000m
    limits:
      memory: 4Gi
      cpu: 2000m
  prometheusSpec:
    secrets:
    - etcd-client-certs
    # Improve retention settings
    retention: 15d
    retentionSize: 90GB
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector:
      matchLabels:
        release: kube-prometheus-stack
    topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          app: kube-prometheus-stack
    securityContext:
      fsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-block
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 50Gi
  ingress:
    enabled: true
    ingressClassName: internal
    annotations:
      kubernetes.io/ingress.class: internal
      internal-dns.alpha.kubernetes.io/enabled: "true"
      cert-manager.io/cluster-issuer: "letsencrypt-production"
      secret.reloader.stakater.com/reload: gateway-monitor
      # Add security headers
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers "X-Frame-Options: SAMEORIGIN";
        more_set_headers "X-Content-Type-Options: nosniff";
        more_set_headers "X-XSS-Protection: 1; mode=block";
    hosts:
    - prometheus.wakkalabs.com
    path: /
    pathType: Prefix
    tls:
    - secretName: prometheus-wakkalabs-tls
      hosts:
      - prometheus.wakkalabs.com

# ================================
# Alertmanager Configuration
# ================================
alertmanager:
  replicas: 1 # For high availability
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1
  service:
    ipFamilyPolicy: SingleStack
    ipFamilies:
    - IPv4
  # Configure Alertmanager to reduce alert noise
  config:
    global:
      resolve_timeout: 5m
      # Add slack_api_url if you want to use Slack
      # slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    route:
      group_by: [ 'job', 'alertname', 'namespace', 'severity' ]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
      # Add inhibition rules to reduce alert noise
      - match:
          severity: critical
        receiver: 'discord-critical'
        continue: true
      - match:
          severity: warning
        receiver: 'discord-warnings'
        # Only send warnings if they persist for 10 minutes
        group_wait: 10m
        # Throttle warnings more aggressively
        repeat_interval: 24h
    # Add inhibition rules
    inhibit_rules:
    # If a critical alert exists, don't send related warnings
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      # Alerts must share these labels to be considered related
      equal: [ 'alertname', 'namespace', 'instance' ]
    receivers:
    - name: 'discord-critical'
      webhook_configs:
      - url: >-
          https://discord.com/api/webhooks/1355431526891786240/5KSnlY0KPdx7dRsSL0yJDaZYNzVktheQz1PXNoX9-eCIlZP2MFZcM67PtFRTRa4ZwJ_a
        send_resolved: true
    - name: 'discord-warnings'
      webhook_configs:
      - url: >-
          https://discord.com/api/webhooks/1355379117692027012/y6Sxl2G-D4hqr8TQVL8rT1HRb1n_h2e8x7nQoNbSDZz_8cGYII2ZVeziLxujkFjkm89X
        send_resolved: true
    # Add a null receiver for silenced alerts
    - name: 'null'
    templates:
    - '/etc/alertmanager/config/*.tmpl'
  templateFiles:
    discord_default.tmpl: |-
      {{ define "discord.default.message" }}
      :bell: **{{ .Status | toUpper }}** {{ if eq .Status "firing" }}:fire:{{ else }}:white_check_mark:{{ end }}
      **Alert:** {{ .CommonAnnotations.summary }}
      **Description:** {{ .CommonAnnotations.description }}
      **Severity:** {{ .CommonLabels.severity | toUpper }}
      **Details:**
      {{ range .Alerts }}
        **Dashboard:** {{ if .Annotations.dashboard }}{{ .Annotations.dashboard }}{{ else }}No dashboard{{ end }}
        **Runbook:** {{ if .Annotations.runbook }}{{ .Annotations.runbook }}{{ else }}No runbook{{ end }}
        **Instance:** {{ .Labels.instance }}
        **Namespace:** {{ .Labels.namespace }}
      {{ end }}
      {{ end }}
  ingress:
    enabled: true
    ingressClassName: internal
    annotations:
      kubernetes.io/ingress.class: internal
      internal-dns.alpha.kubernetes.io/enabled: "true"
      cert-manager.io/cluster-issuer: "letsencrypt-production"
      secret.reloader.stakater.com/reload: gateway-monitor
      # Add security headers
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers "X-Frame-Options: SAMEORIGIN";
        more_set_headers "X-Content-Type-Options: nosniff";
        more_set_headers "X-XSS-Protection: 1; mode=block";
    hosts:
    - alertmanager.wakkalabs.com
    path: /
    pathType: Prefix
    tls:
    - secretName: alertmanager-wakkalabs-tls
      hosts:
      - alertmanager.wakkalabs.com

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    kubeApiserver: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubernetesSystem: true
    kubernetesResources: true
    kubeControllerManager: true
    kubeScheduler: true
    kubernetesStorage: true
    prometheusOperator: true
    # Add custom rule settings to reduce noise
    # Add configmaps for custom rules
  additionalRuleLabels:
    team: platform
  # Silence specific noisy alerts
  disabled:
    KubeControllerManagerDown: false
    KubeSchedulerDown: false
    KubeletTooManyPods: true

kubeControllerManager:
  service:
    ipFamilyPolicy: SingleStack
    ipFamilies:
    - IPv4

kubeScheduler:
  service:
    ipFamilyPolicy: SingleStack
    ipFamilies:
    - IPv4

# ================================
# Grafana Configuration
# ================================
grafana:
  admin:
    existingSecret: grafana-admin-credentials
    userKey: admin-user
    passwordKey: admin-password
  enabled: true
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: "America/Chicago"
  defaultDashboardsEditable: true
  # Add Grafana settings for better security and usability
  securityContext:
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472
  # Configure SMTP for Grafana alerts
  smtp:
    enabled: false # Change to true and configure when ready
    host: "smtp.example.com:587"
    user: "alerts@example.com"
    password: "password" # Use existingSecret instead in production
    from_address: "alerts@example.com"
    from_name: "Grafana Alerts"
  # Enable metrics
  serviceMonitor:
    enabled: true
  # Configure sidecar for auto-importing dashboards
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      folder: /tmp/dashboards
      searchNamespace: ALL
      provider:
        foldersFromFilesStructure: true
    datasources:
      enabled: true
      label: grafana_datasource
      searchNamespace: ALL
  persistence:
    enabled: true
    type: pvc
    accessModes:
    - ReadWriteOnce
    size: 100Gi
    storageClassName: ceph-block
  ingress:
    enabled: true
    ingressClassName: internal
    annotations:
      kubernetes.io/ingress.class: internal
      internal-dns.alpha.kubernetes.io/enabled: "true"
      cert-manager.io/cluster-issuer: "letsencrypt-production"
      secret.reloader.stakater.com/reload: gateway-monitor
      # Add security headers
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers "X-Frame-Options: SAMEORIGIN";
        more_set_headers "X-Content-Type-Options: nosniff";
        more_set_headers "X-XSS-Protection: 1; mode=block";
    hosts:
    - grafana.wakkalabs.com
    path: /
    pathType: Prefix
    tls:
    - secretName: grafana-wakkalabs-tls
      hosts:
      - grafana.wakkalabs.com

  additionalDataSources:
  - name: Tempo
    type: tempo
    uid: tempo
    access: proxy
    url: http://tempo.monitoring.svc.cluster.local:3100
    jsonData:
      httpMethod: GET
      lokiSearch:
        datasourceUid: loki
      tracesToLogs:
        datasourceUid: loki
        tags: [ 'instance', 'pod', 'namespace', 'service.name' ]
        mappedTags: [ { key: 'service.name', value: 'service' } ]
        mapTagNamesEnabled: true
        spanStartTimeShift: '-1h'
        spanEndTimeShift: '1h'
        filterByTraceID: true
        filterBySpanID: true
      serviceMap:
        datasourceUid: prometheus
      nodeGraph:
        enabled: true
      search:
        hide: false
  # Add Loki for log aggregation
  - name: Loki
    type: loki
    uid: loki
    access: proxy
    url: http://loki-gateway.monitoring.svc.cluster.local
    jsonData:
      maxLines: 1000
      derivedFields:
      - datasourceUid: tempo
        matcherRegex: "traceID=(\\w+)"
        name: TraceID
        url: "$${__value.raw}"

  # Add pre-configured dashboards for Loki and additional monitoring
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default
      - name: 'kubernetes'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/kubernetes

  # Define dashboards
  dashboards:
    default:
      # Add custom dashboards here
      kubernetes-cluster-monitoring:
        gnetId: 13770
        revision: 1
        datasource: Prometheus
    kubernetes:
      k8s-node-rsrc-use:
        gnetId: 8171
        revision: 1
        datasource: Prometheus
      logs-dashboard:
        gnetId: 12611
        revision: 1
        datasource: Loki
      # Add Tempo tracing dashboards
      tempo-overview:
        gnetId: 15643
        revision: 1
        datasource: Tempo
      tempo-service-graph:
        gnetId: 14850
        revision: 1
        datasource:
        - name: prometheus
          value: Prometheus
        - name: tempo
          value: Tempo
      application-trace-analysis:
        gnetId: 17836
        revision: 1
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        - name: DS_TEMPO
          value: Tempo

# Add additional exporters
nodeExporter:
  enabled: true
  priorityClassName: system-node-critical
  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 100Mi

# Enable kube-state-metrics
kubeStateMetrics:
  enabled: true
  selfMonitor:
    enabled: true

# Add Loki stack for log management
loki:
  enabled: false # Set to true if you want to deploy Loki with this chart
tempo:
  enabled: true # Enable Tempo for distributed tracing
  replicas: 2 # Run multiple replicas for high availability
  service:
    name: tempo
    port: 3100
  serviceMonitor:
    enabled: true
  ingress:
    enabled: true
    ingressClassName: internal
    annotations:
      kubernetes.io/ingress.class: internal
      internal-dns.alpha.kubernetes.io/enabled: "true"
      cert-manager.io/cluster-issuer: "letsencrypt-production"
      secret.reloader.stakater.com/reload: gateway-monitor
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers "X-Frame-Options: SAMEORIGIN";
        more_set_headers "X-Content-Type-Options: nosniff";
        more_set_headers "X-XSS-Protection: 1; mode=block";
    hosts:
    - tempo.wakkalabs.com
    path: /
    pathType: Prefix
    tls:
    - secretName: tempo-wakkalabs-tls
      hosts:
      - tempo.wakkalabs.com
  tempoQuery:
    enabled: true
    service:
      port: 16686
  # Storage configuration for Tempo
  storage:
    trace:
      backend: local
      local:
        path: /var/tempo/traces
    # Configure appropriate retention
    retention: 168h # 7 days retention
  # Configure persistent volume for storage
  persistence:
    enabled: true
    size: 50Gi
    storageClassName: ceph-block # Using the same storage class as other components
    accessModes:
    - ReadWriteOnce
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  # Configure security context
  securityContext:
    fsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
  # Set the receivers configuration
  receivers:
    jaeger:
      protocols:
        thrift_compact:
          endpoint: 0.0.0.0:6831
        thrift_binary:
          endpoint: 0.0.0.0:6832
        grpc:
          endpoint: 0.0.0.0:14250
        thrift_http:
          endpoint: 0.0.0.0:14268
    zipkin:
      endpoint: 0.0.0.0:9411
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
  # Configure processors for better performance
  processors:
    batch:
      timeout: 1s
      send_batch_size: 1000
  # Configure multi-tenant mode (optional)
  multitenancy:
    enabled: false
  # Configure distributed mode (single binary by default)
  distributor:
    log_received_spans:
      enabled: true
      include_all_attributes: false
