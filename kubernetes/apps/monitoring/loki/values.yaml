loki:
  # Add this line to disable the commonConfig section and prevent the error
  commonConfig: nullk
  auth_enabled: false
  schemaConfig:
    configs:
      - from: 2024-04-01
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: loki_index_
          period: 24h
  storage_config: # This configures Loki's internal storage settings
    aws:

      s3: http://rook-ceph-rgw-ceph-objectstore.rook-ceph.svc.cluster.local
       bucketnames: ceph-chunks
        region: us-east-1
        access_key_id: S7RFA0US3ZM69ZVEIKR
         secret_access_key: b5JsiA2vyY7UHAf2XqRTpo08soE2g0cBWvXiOh6J
         signature_version: v4
      endpoint: http://rook-ceph-rgw-ceph-objectstore.rook-ceph.svc.cluster.local # <-- Add your actual endpoint
      secretAccessKey: b5JsiA2vyY7UHAf2XqRTpo08soE2g0cBWvXiOh6J # <-- Add your actual secret key
      accessKeyId: S7RFA0US3ZM69ZVEIKRN
      insecure: true # Use with caution in production

  ingester:
    chunk_encoding: snappy
  pattern_ingester:
      enabled: true
  limits_config:
    allow_structured_metadata: true
    volume_enabled: true
    retention_period: 672h # 28 days retention
  querier:
      max_concurrent: 4

  storage: # This section often configures chart-level storage details and might be passed to components or helpers
    type: s3
    # The bucketNames here acts configure the chart's understanding,
    # while storage_config.aws.bucketnames configure Loki itself.
    # It seems commonStorageConfig was trying to read from this chart-level section.
    bucketNames:
        chunks: ceph-chunks
        ruler: ceph-ruler
        admin: ceph-admin
    s3: # S3 Access details might also be needed here depending on chart logic
      # You likely need to duplicate some or all of the storage_config.aws
      # settings here as well, depending on the chart's design assumptions.
      # Add endpoint, region, secretAccessKey, accessKeyId, s3ForcePathStyle, insecure etc.
      # Example:
      endpoint: http://rook-ceph-rgw-ceph-objectstore.rook-ceph.svc.cluster.local # <-- Add your actual endpoint
      region: us-east-1
      secretAccessKey: b5JsiA2vyY7UHAf2XqRTpo08soE2g0cBWvXiOh6J # <-- Add your actual secret key
      accessKeyId: S7RFA0US3ZM69ZVEIKRN # <-- Add your actual access key ID
      signatureVersion: v4
      s3ForcePathStyle: true
      insecure: true
      http_config: {} # This might be needed as an empty map

  deploymentMode: Distributed

  # Disable minio storage
  minio:
      enabled: false

  ingester:
    replicas: 3
    zoneAwareReplication:
      enabled: false
  querier:
    replicas: 3
    maxUnavailable: 2
  queryFrontend:
    replicas: 2
    maxUnavailable: 1
  queryScheduler:
    replicas: 2
  distributor:
    replicas: 3
    maxUnavailable: 2
  compactor:
    replicas: 1
  indexGateway:
    replicas: 2
    maxUnavailable: 1

  bloomPlanner:
    replicas: 0
  bloomBuilder:
    replicas: 0
  bloomGateway:
    replicas: 0

  backend:
    replicas: 0
  read:
    replicas: 0
  write: # Enabled by deploymentMode: Distributed and not explicitly disabled with .write.enabled perhaps?
    replicas: 0 # This sets replica count, not necessarily 'enabled' flag chart might check

  singleBinary:
    replicas: 0
