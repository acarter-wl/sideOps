apiVersion: batch/v1
kind: Job
metadata:
  name: ceph-disk-wipe-job-n4
spec:
  template:
    spec:
      restartPolicy: Never
      nodeName: n4
      containers:
        - name: disk-wiper
          image: quay.io/ceph/ceph:v19.2.2  # Using same Ceph version as your cluster
          securityContext:
            privileged: true # Required for direct disk access
          command:
            - "/bin/bash"
            - "-c"
            - |
              # Define the target disk - this needs to be adjusted based on your environment
              # IMPORTANT: Be extremely careful with this value!
              DISK="/dev/sdb"  # Same disk as in your Ceph config

              echo "Starting Ceph disk wipe procedure on $DISK"

              # Safety check to prevent accidental wiping of system disk
              if [[ "$DISK" == "/dev/sda" || "$DISK" == "/dev/vda" ]]; then
                echo "ERROR: Refusing to wipe potential system disk $DISK"
                exit 1
              fi

              # First use Ceph tools to properly zap the disk
              echo "Using ceph-volume to zap the disk"
              ceph-volume lvm zap --destroy $DISK

              # Remove any Ceph data directory
              echo "Removing Ceph data directory"
              rm -rf /var/lib/ceph/osd/*

              # Use wipefs to remove all filesystem signatures
              echo "Removing all filesystem signatures with wipefs"
              wipefs -a $DISK

              # Remove partition tables
              echo "Removing partition tables"
              sgdisk --zap-all $DISK

              # Zero out the beginning of the disk to remove any Ceph metadata
              echo "Zeroing beginning of disk"
              dd if=/dev/zero of=$DISK bs=1M count=100 oflag=direct

              # Zero out specific Ceph OSD areas
              echo "Zeroing Ceph metadata areas"
              # First 10MB
              dd if=/dev/zero of=$DISK bs=1M count=10 oflag=direct
              # Last 10MB
              dd if=/dev/zero of=$DISK bs=1M count=10 seek=$((`blockdev --getsz $DISK` / 2048 - 10)) oflag=direct

              # Zero the superblock at 4k offset (Ceph stores some data here)
              echo "Zeroing superblock"
              dd if=/dev/zero of=$DISK bs=1k count=1 seek=4 oflag=direct

              # Additional step for completely wiping LUKS/dmcrypt if used by Ceph
              echo "Removing any LUKS/dmcrypt headers"
              dd if=/dev/zero of=$DISK bs=512 count=20480 oflag=direct

              # Inform the OS of partition table changes
              echo "Updating kernel partition table"
              partprobe $DISK || true

              # Handle the fsid area specifically (Ceph stores its fsid here)
              echo "Removing Ceph fsid area"
              dd if=/dev/zero of=$DISK bs=4k count=1 seek=1 oflag=direct

              # Verify disk is clean
              echo "Verifying disk is clean"
              if wipefs -n $DISK | grep -q "signature"; then
                echo "WARNING: Disk still has signatures after wiping"
                wipefs -n $DISK
              else
                echo "Disk appears to be clean"
              fi

              echo "Ceph disk wipe completed successfully"
